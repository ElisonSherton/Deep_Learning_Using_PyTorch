{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this Notebook aims to use Tensorboard to visualize the performance metrics and the learning of the network across epochs for a supervised learning dataset of digits available in `sklearn.datasets`, it also covers a few more topics which are mentioned below.\n",
    "- Define your own Dataset class to load data as batchwise tensors. Refer [How to create a custom dataloader](https://medium.com/analytics-vidhya/writing-a-custom-dataloader-for-a-simple-neural-network-in-pytorch-a310bea680af) and the notebook below for the same.\n",
    "\n",
    "- Visualize metrics and parameter learnings in tensorboard.\n",
    "\n",
    "- Perform hyperparameter tuning using cartesian product and an OOP implementation (RunBuilder class).\n",
    "\n",
    "- Organize the training process using OOP and save the results in an ordered fashion using RunManager class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important package\n",
    "import torch\n",
    "\n",
    "# Package with miscellaneous capabilities for loading image data, transforming etc.\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# To define the network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim         # To define the optimizer to study the class\n",
    "\n",
    "# Basic data related operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For loading the dataset and for evaluation purposes\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# For writing to a tensorboard file.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# To compute the cartesian product in hyperparameter tuning\n",
    "from itertools import product\n",
    "\n",
    "# To create a named-tuple in RunBuilder class for using `.` notation to access hyperparameters easily.\n",
    "from collections import namedtuple\n",
    "\n",
    "import time   # To time the epochs and runs across different hyperparameters\n",
    "import json   # To write to a json file\n",
    "from IPython.display import display, clear_output   # To dynamically update the output in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "B_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 150\n",
    "RO = 2 # Round off upto RO digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install tensorboard, you can simply do \n",
    "\n",
    "`!pip install tensorboard` \n",
    "\n",
    "from this notebook. If it's already installed, uninstall and reinstall to get the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = load_digits()\n",
    "X_train, X_ts, y_train, y_ts = train_test_split(bc.data, bc.target, test_size = 0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_ts, y_ts, test_size = 0.333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(X, y):\n",
    "    return np.hstack((X, y.reshape(y.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_target(X_train, y_train)\n",
    "val = add_target(X_val, y_val)\n",
    "test = add_target(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining your own dataloader\n",
    "\n",
    "torch provides `Dataset` and `Dataloader` classes in order to help you feed the data to the network which you can tweak as you like. If you want to define your own way of feeding data, you'll have to inherit the `Dataset` class and implement the `__getitem__` and `__len__` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datset(Dataset):\n",
    "    \n",
    "    # Convert the data into a float tensor\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Last column is the target column\n",
    "        target = self.data[index][-1].long()\n",
    "        \n",
    "        # All other predictor columns (i.e. remove the target column)\n",
    "        data_val = self.data[index][:-1]\n",
    "        \n",
    "        # Return the predictor variables and the target variable\n",
    "        return data_val, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets to pass to the dataloader object\n",
    "train_set = datset(train)\n",
    "val_set = datset(val)\n",
    "test_set = datset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders to load data in batches\n",
    "train_loader = DataLoader(train_set, batch_size = B_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = B_SIZE)\n",
    "test_loader = DataLoader(test_set, batch_size = B_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification with certain number of predictors\n",
    "input_size = train_set[0][0].size()[0]\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First Dense Layer\n",
    "        self.fc1 = nn.Linear(in_features = input_size, out_features = 50)\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(in_features = 50, out_features = output_size)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # Input\n",
    "        t = t\n",
    "        \n",
    "        # Dense Layer 1\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # Output Layer\n",
    "        t = self.out(t)\n",
    "        t = F.softmax(t, dim = 1)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate validation accuracy and validation loss\n",
    "def validate(model, loader):\n",
    "    # Define containers to accumulate loss and correct predictions\n",
    "    total_loss = 0.\n",
    "    correct_predictions = 0.\n",
    "    \n",
    "    # Since we're only predicting, we don't need to compute the gradients\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    for batch in loader:\n",
    "        predictors, target = batch\n",
    "        \n",
    "        # Do a forward pass\n",
    "        predictions = model(predictors)\n",
    "        \n",
    "        # Compute and accumulate the loss\n",
    "        total_loss += F.cross_entropy(predictions, target).item()\n",
    "        \n",
    "        # Find out the argmax to get the category of each entry\n",
    "        predictions = predictions.argmax(dim = 1)\n",
    "        \n",
    "        # Get the total number of correct pedictions in the batch and accumulate the same\n",
    "        correct_predictions += predictions.eq(target).sum().item()\n",
    "    \n",
    "    # Switch on the gradient computing \n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # Return the loss and accuracy\n",
    "    acc = 100 * correct_predictions / (loader.batch_size * len(loader))\n",
    "    \n",
    "    return (np.round(total_loss , RO), np.round(acc, RO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning and Tensorboard Visualization\n",
    "\n",
    "We can perform hyperparameter tuning using the following methods and visualize it in tensorboard. Then we can pick the best hyperparameters for building our network and use it.\n",
    "\n",
    "**Creating a grid of values**\n",
    "- Individually define lists for all the parameters which you wanna use to build the network architecture.\n",
    "\n",
    "- Use Cartesian product to create an assortment of all those pairs together.\n",
    "\n",
    "```python\n",
    "params = dict(\n",
    "LR_values = list(np.linspace(0.01, 0.04, 4)),\n",
    "BATCH_SIZE_values = [10, 32, 64]\n",
    ")\n",
    "\n",
    "# Make a list of the lists of values\n",
    "param_values = [val for val in params.values()]\n",
    "\n",
    "# Iterate over the product\n",
    "from itertools import product\n",
    "for i, j in product(*param_values):\n",
    "    print(i, j)\n",
    "```\n",
    "- Iterate over the parameters in these list to obtain the performance parameters and store them.\n",
    "\n",
    "- Pick those parameters which gave the best value.\n",
    "\n",
    "**Storing data in a logfile to visualize in tensorboard**\n",
    "\n",
    "Use the `SummaryWriter` class in order to create a logfile to store the details of runs for every epoch and for every combination of tried out hyperparameters.\n",
    "\n",
    "Use the `comment` argument of `SummaryWriter` class to accomplish this purpose.\n",
    "\n",
    "```python\n",
    "# Define a writer with respective params in comment\n",
    "cm = f\"Learning Rate- {LR} Batch Size- {BATCH_SZ}\"\n",
    "tb = SummaryWriter(comment = cm)\n",
    "\n",
    "# Add a scalar to the written file\n",
    "tb.add_scalar(\"Loss\", epoch_loss, e)\n",
    "\n",
    "# Add a histogram (weights, biases, gradients) to the written file\n",
    "tb.add_histogram(name, weight, e)\n",
    "\n",
    "# Close the file before quitting the program\n",
    "tb.close()\n",
    "\n",
    "```\n",
    "\n",
    "Save the training loss and training and testing accuracies as scalars and the weights of the network layers as histograms. Then you can open command prompt, navigate to the folder which contains these logfiles (generally logfiles are kept in the runs folder, so you can navigate to the folder containing the runs folder) and type the following command in anaconda prompt\n",
    "\n",
    "`tensorboard --logdir=runs`\n",
    "\n",
    "It will give you a server which you open in a web browser and there you can find plots for all your parameters. \n",
    "\n",
    "A few examples of the same are shown in the snapshot below\n",
    "![](./images/Tensorboard_graphs.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "LR_values = list(np.linspace(0.01, 0.04, 4)),\n",
    "BATCH_SIZE_values = [10, 32, 64]\n",
    ")\n",
    "\n",
    "param_values = [val for val in params.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.01 Batch Size- 10\n",
      "Epoch: 0  Train Loss: 239.08 Train Accuracy: 56.25 Validation Loss: 11.08 Validation Accuracy: 56.77\n",
      "Epoch: 149  Train Loss: 199.4 Train Accuracy: 87.83 Validation Loss: 9.74 Validation Accuracy: 78.65\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.01 Batch Size- 32\n",
      "Epoch: 0  Train Loss: 75.17 Train Accuracy: 59.03 Validation Loss: 10.44 Validation Accuracy: 68.49\n",
      "Epoch: 149  Train Loss: 59.49 Train Accuracy: 97.3 Validation Loss: 8.97 Validation Accuracy: 90.36\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.01 Batch Size- 64\n",
      "Epoch: 0  Train Loss: 39.88 Train Accuracy: 46.38 Validation Loss: 10.94 Validation Accuracy: 60.68\n",
      "Epoch: 149  Train Loss: 29.32 Train Accuracy: 99.52 Validation Loss: 8.89 Validation Accuracy: 91.67\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.02 Batch Size- 10\n",
      "Epoch: 0  Train Loss: 297.85 Train Accuracy: 9.79 Validation Loss: 14.16 Validation Accuracy: 9.9\n",
      "Epoch: 149  Train Loss: 298.0 Train Accuracy: 9.63 Validation Loss: 14.16 Validation Accuracy: 9.9\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.02 Batch Size- 32\n",
      "Epoch: 0  Train Loss: 83.17 Train Accuracy: 37.63 Validation Loss: 12.33 Validation Accuracy: 37.5\n",
      "Epoch: 149  Train Loss: 75.45 Train Accuracy: 57.28 Validation Loss: 11.39 Validation Accuracy: 52.6\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.02 Batch Size- 64\n",
      "Epoch: 0  Train Loss: 38.37 Train Accuracy: 53.86 Validation Loss: 10.94 Validation Accuracy: 60.16\n",
      "Epoch: 149  Train Loss: 30.67 Train Accuracy: 92.76 Validation Loss: 9.23 Validation Accuracy: 86.98\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.03 Batch Size- 10\n",
      "Epoch: 0  Train Loss: 296.86 Train Accuracy: 10.5 Validation Loss: 14.19 Validation Accuracy: 9.11\n",
      "Epoch: 149  Train Loss: 296.8 Train Accuracy: 10.58 Validation Loss: 14.19 Validation Accuracy: 9.11\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.03 Batch Size- 32\n",
      "Epoch: 0  Train Loss: 82.74 Train Accuracy: 38.66 Validation Loss: 11.91 Validation Accuracy: 44.79\n",
      "Epoch: 149  Train Loss: 72.65 Train Accuracy: 64.04 Validation Loss: 10.83 Validation Accuracy: 61.46\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.03 Batch Size- 64\n",
      "Epoch: 0  Train Loss: 45.8 Train Accuracy: 16.55 Validation Loss: 13.49 Validation Accuracy: 20.05\n",
      "Epoch: 149  Train Loss: 39.87 Train Accuracy: 46.7 Validation Loss: 11.69 Validation Accuracy: 47.92\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.04 Batch Size- 10\n",
      "Epoch: 0  Train Loss: 296.46 Train Accuracy: 10.74 Validation Loss: 14.21 Validation Accuracy: 8.33\n",
      "Epoch: 149  Train Loss: 296.7 Train Accuracy: 10.66 Validation Loss: 14.21 Validation Accuracy: 8.33\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.04 Batch Size- 32\n",
      "Epoch: 0  Train Loss: 94.72 Train Accuracy: 8.99 Validation Loss: 14.23 Validation Accuracy: 8.33\n",
      "Epoch: 149  Train Loss: 94.65 Train Accuracy: 9.47 Validation Loss: 14.23 Validation Accuracy: 8.33\n",
      "\n",
      "The set of parameters for testing are as follows: \n",
      "\n",
      "Learning Rate- 0.04 Batch Size- 64\n",
      "Epoch: 0  Train Loss: 47.05 Train Accuracy: 10.66 Validation Loss: 14.19 Validation Accuracy: 9.11\n",
      "Epoch: 149  Train Loss: 47.11 Train Accuracy: 10.58 Validation Loss: 14.19 Validation Accuracy: 9.11\n"
     ]
    }
   ],
   "source": [
    "for LR, BATCH_SZ in product(*param_values): \n",
    "    cm = f\"Learning Rate- {LR} Batch Size- {BATCH_SZ}\"\n",
    "    print(\"\\nThe set of parameters for testing are as follows: \\n\")\n",
    "    print(cm)\n",
    "    # Create an instance of the network\n",
    "    model = Network()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "    # Implement the training loop\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # Define the loader\n",
    "    train_loader = DataLoader(train_set, batch_size = BATCH_SZ)\n",
    "    \n",
    "    # Define the SummaryWriter to write in a log file\n",
    "    tb = SummaryWriter(comment = cm)\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "        epoch_loss = 0.\n",
    "        epoch_correct_predictions = 0.\n",
    "\n",
    "        for batch in train_loader:\n",
    "            predictors, target = batch        \n",
    "\n",
    "            # Forward propagation\n",
    "            predictions = model(predictors)\n",
    "\n",
    "            # Calculate  and accumulate the loss\n",
    "            loss = F.cross_entropy(predictions, target)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Empty the gradients before accumulating them\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backpropagation i.e. find out the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Modify the weights of the model\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get the index that corresponds to the class\n",
    "            predictions = predictions.argmax(dim = 1)\n",
    "\n",
    "            # Find out the number of correct predictions for this batch and accumulate them\n",
    "            epoch_correct_predictions += predictions.eq(target).sum().item()\n",
    "\n",
    "        val_loss, val_accuracy = validate(model, val_loader)\n",
    "        \n",
    "        train_accuracy = 100 * epoch_correct_predictions / len(train_set)\n",
    "        \n",
    "        if ((e == 0) or (e == EPOCHS - 1)):\n",
    "            print(f\"Epoch: {e}  Train Loss: {np.round(epoch_loss, RO)} Train Accuracy: {np.round(train_accuracy, RO)} Validation Loss: {val_loss} Validation Accuracy: {val_accuracy}\")\n",
    "    \n",
    "        tb.add_scalar(\"Loss\", epoch_loss, e)\n",
    "        tb.add_scalar(\"Train Accuracy\", train_accuracy, e)\n",
    "        tb.add_scalar(\"Validation Accuracy\", val_accuracy, e)\n",
    "\n",
    "        for name, weight in model.named_parameters():\n",
    "            tb.add_histogram(name, weight, e)\n",
    "            tb.add_histogram(f\"{name}.grad\", weight.grad, e)\n",
    "    \n",
    "    # Close the writer object    \n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing Code\n",
    "## RunBuilder Class\n",
    "\n",
    "We can create a class in order to store the values of cartesian product of different hyperparameters which we did above. It helps clean the code up and create order, improve readability and add modularity to the code.\n",
    "\n",
    "What will this class do?\n",
    "\n",
    "It will help to get individual named tuples which could be used to iterate over and obtain results for different hyperparameters. This is how we can do it:\n",
    "\n",
    "```python\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        # Creates a named tuple which we can use to access values in organized way of .notation\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        # Create a container to hold all the combination of params\n",
    "        runs = []\n",
    "        \n",
    "        # Compute the Cartesian product of parameters\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "        \n",
    "        return runs\n",
    "```\n",
    "\n",
    "**Why Static Method?**\n",
    "\n",
    "We do not want to create an instance of the class because it has a method which will be used by one and all identically to create groups of hyperparameter values.\n",
    "\n",
    "We could then access the class method as follows:\n",
    "```python\n",
    "params = OrderedDict(\n",
    "    lr = [.01, .001]\n",
    "    ,batch_size = [1000, 10000]\n",
    ")\n",
    "\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    comment = f'-{run}'\n",
    "```\n",
    "\n",
    "How is this better than unpacking values from a simple cartesian product? In the `for` loop, you've to unpack the values manually and do stuff whereas here, the class' method handles that. \n",
    "\n",
    "## RunManager Class\n",
    "\n",
    "We can see the code above is cluttered and has a lot of stuff written in chunks which could be modularized and organized nicely. Let's implement that.\n",
    "\n",
    "A brief on what the attributes of the RunManager class are and what are each of the methods doing:\n",
    "\n",
    "`__init__`: It is initializing all the instance varaibles of the class which are as follows:\n",
    "\n",
    "   - `e`: It is an instance of the class Epoch which holds information about an epoch's count, it's loss, it's number of correct predictions, and the time at which that epoch started.\n",
    "\n",
    "   - `run_params`: It's a named tuple returned by the `RunBuilder` class which holds the values of parameters with which the run has happened.\n",
    "\n",
    "   - `run_count`: It's a running count of which named tuple or which set of hyperparameters am I using for tuning currently.\n",
    "\n",
    "   - `run_data`: It's a container to hold all the results related to a run as we step through different values of run_params\n",
    "\n",
    "   - `run_start_time`: When we started experimenting with a particular value of run_params.\n",
    "\n",
    "   - `network`: It's the neural network architecture with weights.\n",
    "   \n",
    "   - `loader`: It's the loader through which batches of data will be passed through the network.\n",
    "   \n",
    "   - `tb`: It's the tensorboard object used to write data to runs folder for visualizing results in TensorBoard\n",
    "   \n",
    "`begin_run`: It is a method which is called at the begining of a run before the loop of epochs. It does the following things.\n",
    "   \n",
    "   - Remembers the start time of the run.\n",
    "   \n",
    "   - Sets the hyperparameters for a run.\n",
    "   \n",
    "   - Keeps track of how many run combinations are happening. Increments by 1 every time this method is called.\n",
    "   \n",
    "   - Defines the loader based on batch_size which may be a potential hyperparameter.\n",
    "   \n",
    "   - Instantiate a SummaryWriter object to store the results of all the runs.\n",
    "\n",
    "`end_run`: When a run is over, it resets the epoch count and closes the SummaryWriter object.\n",
    "\n",
    "`begin_epoch`: It is called at the beginning of each epoch. What it does is increments the epoch count, registers the start time of an epoch and resets the loss and num_correct attributes of the Epoch instance to zero.\n",
    "\n",
    "`end_epoch`: It does a lot of things. They're as follows.\n",
    "\n",
    "- It clocks in the time required for one epoch to complete. Also it calculates the runtime which ain't right except for the last epoch of a run but pardon the inconvenience caused.\n",
    "\n",
    "- Compute the total loss and accuracy. \n",
    "\n",
    "- Write these parameters to tensorboard.\n",
    "\n",
    "- Store the values corresponding to the epoch finished in the `run_data` container \n",
    "\n",
    "`track_loss`: It computes the loss at the end of every epoch and stores it to the Epoch instance attribute.\n",
    "\n",
    "`track_num_correct`: It computes the number of correct predictions and stores it to the Epoch instance attribute.\n",
    "\n",
    "`save`: It saves the `run_data` instance variables in an orderly fashion to a csv and json file respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define the RunBuilder class\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        # Creates a named tuple which we can use to access values in organized way of .notation\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        # Create a container to hold all the combination of params\n",
    "        runs = []\n",
    "        \n",
    "        # Compute the Cartesian product of parameters\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "        \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define the epoch class\n",
    "class Epoch():\n",
    "    def __init__(self):\n",
    "        # Keeps track of which epoch it is\n",
    "        self.count = 0\n",
    "        # Keeps track of the loss\n",
    "        self.loss = 0\n",
    "        # Keeps track of the number of correct predictions\n",
    "        self.num_correct = 0\n",
    "        # When to start\n",
    "        self.start_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "code_folding": [
     5,
     15,
     24,
     28
    ]
   },
   "outputs": [],
   "source": [
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        self.e = Epoch()\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "    \n",
    "    def begin_run(self, run, network, loader):\n",
    "        self.run_start_time = time.time()\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment = f\"-{run}\")\n",
    "#         self.tb.add_graph(network)\n",
    "    \n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.e.count = 0\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        self.e.start_time = time.time()\n",
    "        self.e.count += 1\n",
    "        self.e.loss = 0\n",
    "        self.e.num_correct = 0\n",
    "        \n",
    "    def end_epoch(self):\n",
    "        epoch_duration = time.time() - self.e.start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "        \n",
    "        loss = self.e.loss /len(self.loader.dataset)\n",
    "        accuracy = self.e.num_correct / (len(self.loader.dataset))\n",
    "        \n",
    "        self.tb.add_scalar('Loss', loss, self.e.count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.e.count)\n",
    "        \n",
    "        for name, param in network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.e.count)\n",
    "            self.tb.add_histogram(f\"{name}.grad\", param.grad, self.e.count)\n",
    "            \n",
    "        results = {'Run': self.run_count, \n",
    "                   'Epoch': self.e.count, \n",
    "                   'Loss':loss,\n",
    "                   'Accuracy': accuracy,\n",
    "                   'Epoch Duration': epoch_duration,\n",
    "                   'Run Duration': run_duration}\n",
    "        \n",
    "        for k, v in self.run_params._asdict().items(): \n",
    "            results[k] = v\n",
    "        \n",
    "        self.run_data.append(results)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "        clear_output(wait = True)\n",
    "        display(df)\n",
    "        \n",
    "    def track_loss(self, loss):\n",
    "        self.e.loss += loss.item() * self.loader.batch_size\n",
    "    \n",
    "    def track_num_correct(self, pred, labels):\n",
    "        self.e.num_correct += pred.argmax(dim = 1).eq(labels).sum().item()\n",
    "        \n",
    "    def save(self, fileName):\n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data, orient='columns'\n",
    "        ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    lr = [.01, 0.02]\n",
    "    ,batch_size = [100, 200]\n",
    "    ,shuffle = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Epoch Duration</th>\n",
       "      <th>Run Duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.368076</td>\n",
       "      <td>0.170247</td>\n",
       "      <td>0.068634</td>\n",
       "      <td>0.140598</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.336621</td>\n",
       "      <td>0.206046</td>\n",
       "      <td>0.115923</td>\n",
       "      <td>0.343686</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.316710</td>\n",
       "      <td>0.214797</td>\n",
       "      <td>0.129149</td>\n",
       "      <td>0.550384</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.296158</td>\n",
       "      <td>0.237868</td>\n",
       "      <td>0.084952</td>\n",
       "      <td>0.721663</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.274956</td>\n",
       "      <td>0.278441</td>\n",
       "      <td>0.111935</td>\n",
       "      <td>0.918147</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.407680</td>\n",
       "      <td>0.095465</td>\n",
       "      <td>0.078117</td>\n",
       "      <td>0.448470</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.373144</td>\n",
       "      <td>0.162291</td>\n",
       "      <td>0.079953</td>\n",
       "      <td>0.650438</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.327235</td>\n",
       "      <td>0.228321</td>\n",
       "      <td>0.087910</td>\n",
       "      <td>0.838034</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.286817</td>\n",
       "      <td>0.264916</td>\n",
       "      <td>0.086984</td>\n",
       "      <td>1.005302</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.232415</td>\n",
       "      <td>0.383453</td>\n",
       "      <td>0.118281</td>\n",
       "      <td>1.197232</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.590497</td>\n",
       "      <td>0.078759</td>\n",
       "      <td>0.070885</td>\n",
       "      <td>0.137852</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.561239</td>\n",
       "      <td>0.155927</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.294547</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.531123</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.072959</td>\n",
       "      <td>0.484437</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.515115</td>\n",
       "      <td>0.222753</td>\n",
       "      <td>0.089298</td>\n",
       "      <td>0.666629</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.488378</td>\n",
       "      <td>0.238663</td>\n",
       "      <td>0.078753</td>\n",
       "      <td>0.822643</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.540725</td>\n",
       "      <td>0.171838</td>\n",
       "      <td>0.110936</td>\n",
       "      <td>0.202883</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.511451</td>\n",
       "      <td>0.231504</td>\n",
       "      <td>0.115937</td>\n",
       "      <td>0.413765</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.482996</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>0.066363</td>\n",
       "      <td>0.583712</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.454180</td>\n",
       "      <td>0.330151</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2.425282</td>\n",
       "      <td>0.351631</td>\n",
       "      <td>0.085094</td>\n",
       "      <td>0.937357</td>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.383115</td>\n",
       "      <td>0.126492</td>\n",
       "      <td>0.066555</td>\n",
       "      <td>0.105533</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2.300842</td>\n",
       "      <td>0.265712</td>\n",
       "      <td>0.082952</td>\n",
       "      <td>0.301120</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.246166</td>\n",
       "      <td>0.303103</td>\n",
       "      <td>0.076956</td>\n",
       "      <td>0.480004</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.209580</td>\n",
       "      <td>0.326173</td>\n",
       "      <td>0.086869</td>\n",
       "      <td>0.661145</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2.163361</td>\n",
       "      <td>0.407319</td>\n",
       "      <td>0.136894</td>\n",
       "      <td>0.909928</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.390906</td>\n",
       "      <td>0.102625</td>\n",
       "      <td>0.085949</td>\n",
       "      <td>0.139918</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.343265</td>\n",
       "      <td>0.191726</td>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.313823</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2.290136</td>\n",
       "      <td>0.261734</td>\n",
       "      <td>0.088430</td>\n",
       "      <td>0.502310</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.216072</td>\n",
       "      <td>0.406523</td>\n",
       "      <td>0.075957</td>\n",
       "      <td>0.700197</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.144038</td>\n",
       "      <td>0.471758</td>\n",
       "      <td>0.084608</td>\n",
       "      <td>0.884073</td>\n",
       "      <td>0.02</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.550345</td>\n",
       "      <td>0.156722</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.192888</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2.500608</td>\n",
       "      <td>0.221161</td>\n",
       "      <td>0.081952</td>\n",
       "      <td>0.367787</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2.462559</td>\n",
       "      <td>0.265712</td>\n",
       "      <td>0.075942</td>\n",
       "      <td>0.571669</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.436691</td>\n",
       "      <td>0.284010</td>\n",
       "      <td>0.077945</td>\n",
       "      <td>0.781551</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2.412410</td>\n",
       "      <td>0.297534</td>\n",
       "      <td>0.088948</td>\n",
       "      <td>0.998425</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2.567632</td>\n",
       "      <td>0.093874</td>\n",
       "      <td>0.064625</td>\n",
       "      <td>0.120604</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2.519170</td>\n",
       "      <td>0.222753</td>\n",
       "      <td>0.087469</td>\n",
       "      <td>0.333064</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2.473414</td>\n",
       "      <td>0.260939</td>\n",
       "      <td>0.076957</td>\n",
       "      <td>0.523955</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2.431086</td>\n",
       "      <td>0.291965</td>\n",
       "      <td>0.073959</td>\n",
       "      <td>0.723840</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2.376337</td>\n",
       "      <td>0.408115</td>\n",
       "      <td>0.069962</td>\n",
       "      <td>0.923725</td>\n",
       "      <td>0.02</td>\n",
       "      <td>200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Run  Epoch      Loss  Accuracy  Epoch Duration  Run Duration    lr  \\\n",
       "0     1      1  2.368076  0.170247        0.068634      0.140598  0.01   \n",
       "1     1      2  2.336621  0.206046        0.115923      0.343686  0.01   \n",
       "2     1      3  2.316710  0.214797        0.129149      0.550384  0.01   \n",
       "3     1      4  2.296158  0.237868        0.084952      0.721663  0.01   \n",
       "4     1      5  2.274956  0.278441        0.111935      0.918147  0.01   \n",
       "5     2      1  2.407680  0.095465        0.078117      0.448470  0.01   \n",
       "6     2      2  2.373144  0.162291        0.079953      0.650438  0.01   \n",
       "7     2      3  2.327235  0.228321        0.087910      0.838034  0.01   \n",
       "8     2      4  2.286817  0.264916        0.086984      1.005302  0.01   \n",
       "9     2      5  2.232415  0.383453        0.118281      1.197232  0.01   \n",
       "10    3      1  2.590497  0.078759        0.070885      0.137852  0.01   \n",
       "11    3      2  2.561239  0.155927        0.078575      0.294547  0.01   \n",
       "12    3      3  2.531123  0.196500        0.072959      0.484437  0.01   \n",
       "13    3      4  2.515115  0.222753        0.089298      0.666629  0.01   \n",
       "14    3      5  2.488378  0.238663        0.078753      0.822643  0.01   \n",
       "15    4      1  2.540725  0.171838        0.110936      0.202883  0.01   \n",
       "16    4      2  2.511451  0.231504        0.115937      0.413765  0.01   \n",
       "17    4      3  2.482996  0.295943        0.066363      0.583712  0.01   \n",
       "18    4      4  2.454180  0.330151        0.081367      0.752700  0.01   \n",
       "19    4      5  2.425282  0.351631        0.085094      0.937357  0.01   \n",
       "20    5      1  2.383115  0.126492        0.066555      0.105533  0.02   \n",
       "21    5      2  2.300842  0.265712        0.082952      0.301120  0.02   \n",
       "22    5      3  2.246166  0.303103        0.076956      0.480004  0.02   \n",
       "23    5      4  2.209580  0.326173        0.086869      0.661145  0.02   \n",
       "24    5      5  2.163361  0.407319        0.136894      0.909928  0.02   \n",
       "25    6      1  2.390906  0.102625        0.085949      0.139918  0.02   \n",
       "26    6      2  2.343265  0.191726        0.098947      0.313823  0.02   \n",
       "27    6      3  2.290136  0.261734        0.088430      0.502310  0.02   \n",
       "28    6      4  2.216072  0.406523        0.075957      0.700197  0.02   \n",
       "29    6      5  2.144038  0.471758        0.084608      0.884073  0.02   \n",
       "30    7      1  2.550345  0.156722        0.066960      0.192888  0.02   \n",
       "31    7      2  2.500608  0.221161        0.081952      0.367787  0.02   \n",
       "32    7      3  2.462559  0.265712        0.075942      0.571669  0.02   \n",
       "33    7      4  2.436691  0.284010        0.077945      0.781551  0.02   \n",
       "34    7      5  2.412410  0.297534        0.088948      0.998425  0.02   \n",
       "35    8      1  2.567632  0.093874        0.064625      0.120604  0.02   \n",
       "36    8      2  2.519170  0.222753        0.087469      0.333064  0.02   \n",
       "37    8      3  2.473414  0.260939        0.076957      0.523955  0.02   \n",
       "38    8      4  2.431086  0.291965        0.073959      0.723840  0.02   \n",
       "39    8      5  2.376337  0.408115        0.069962      0.923725  0.02   \n",
       "\n",
       "    batch_size  shuffle  \n",
       "0          100     True  \n",
       "1          100     True  \n",
       "2          100     True  \n",
       "3          100     True  \n",
       "4          100     True  \n",
       "5          100    False  \n",
       "6          100    False  \n",
       "7          100    False  \n",
       "8          100    False  \n",
       "9          100    False  \n",
       "10         200     True  \n",
       "11         200     True  \n",
       "12         200     True  \n",
       "13         200     True  \n",
       "14         200     True  \n",
       "15         200    False  \n",
       "16         200    False  \n",
       "17         200    False  \n",
       "18         200    False  \n",
       "19         200    False  \n",
       "20         100     True  \n",
       "21         100     True  \n",
       "22         100     True  \n",
       "23         100     True  \n",
       "24         100     True  \n",
       "25         100    False  \n",
       "26         100    False  \n",
       "27         100    False  \n",
       "28         100    False  \n",
       "29         100    False  \n",
       "30         200     True  \n",
       "31         200     True  \n",
       "32         200     True  \n",
       "33         200     True  \n",
       "34         200     True  \n",
       "35         200    False  \n",
       "36         200    False  \n",
       "37         200    False  \n",
       "38         200    False  \n",
       "39         200    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = RunManager()\n",
    "for run in RunBuilder().get_runs(params):\n",
    "    \n",
    "    network = Network()\n",
    "    \n",
    "    loader = DataLoader(train_set, batch_size = run.batch_size, shuffle = run.shuffle)\n",
    "    \n",
    "    optimizer = optim.SGD(network.parameters(), lr = run.lr)\n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(5):\n",
    "        m.begin_epoch()\n",
    "        \n",
    "        for batch in loader:\n",
    "            predictors, target = batch\n",
    "            \n",
    "            predictions = network(predictors)\n",
    "            \n",
    "            loss = F.cross_entropy(predictions, target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(predictions, target)\n",
    "        \n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "\n",
    "m.save('results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
