{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Tensors\n",
    "\n",
    "\n",
    "\n",
    "This notebook explores the tensor datatype in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of a tensor\n",
    "\n",
    "A tensor object has by default the following three attributes:\n",
    "\n",
    "A Datatype - Specifying the type of the class which an object belongs to.\n",
    "```python\n",
    "print(t.dtype)\n",
    "```\n",
    "A Device - Whether this object lives on the CPU or the GPU.\n",
    "```python\n",
    "print(t.device)\n",
    "```\n",
    "Layout - How the data is stored internally. Not necessarily important to know. Default is good enough.\n",
    "```python\n",
    "print(t.layout)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch tensors have a datatype and the operations in tensors require the datatype of two tensors to be of the same type. It is not possible to operate on tensors with different datatypes.\n",
    "\n",
    "A torch tensor that lives in a gpu vs a torch tensor that lives in cpu has two different datatypes. And it's not possible to operate on them in this state. Either use the `.device('cuda')` to get the cpu tensor on the gpu or vice versa in order to make it happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensor with existing data.\n",
    "\n",
    "There are several ways of creating a tensor with existing data like numpy arrays. They are as follows:\n",
    "\n",
    "```python\n",
    "torch.Tensor(data)\n",
    "torch.tensor(data)\n",
    "torch.as_tensor(data)\n",
    "torch.from_numpy(data)\n",
    "```\n",
    "data is a numpy array which is already defined in memory.\n",
    "\n",
    "**Note:** The first one is different because it's made using the constructor in the Tensor class as if you provide it an integer data or whatever data, it'll interpret it as float data whereas others will more or less be representative dtype of the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensor with predefined functions\n",
    "\n",
    "Sometimes, we need to get an identity matrix or a matrix with all zeros or all ones etc. We can use the following functions for the same.\n",
    "\n",
    "```python\n",
    "torch.eye(n)\n",
    "torch.zeros(m, n)    # For a tensor of all zeros\n",
    "torch.ones(m, n)     # For a tensor of all ones\n",
    "torch.rand(m, n)     # For a tensor of all random numbers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1446, 0.5538, 0.0546],\n",
       "        [0.0184, 0.3180, 0.2586]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Sharing and Copying\n",
    "\n",
    "The two methods `torch.tensor` and `torch.Tensor` work copy the information from a numpy array into a tensor and a separate memory space is made available to them. OTOH, the methods `torch.as_tensor` and `torch.from_numpy` only create references which causes changes to reflect in the destination if they've happened in the source. i.e. If you change the numpy array, then the tensors created from that array will be modified if they're created using `as_tensor` or `from_numpy` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ones((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten, Reshape, Squeeze, Concatenate\n",
    "\n",
    "To check the size of a tensor you can use\n",
    "```python\n",
    "t.size()\n",
    "t.shape\n",
    "```\n",
    "\n",
    "To check the rank of a tensor, you can use \n",
    "```python\n",
    "len(t.size())\n",
    "```\n",
    "\n",
    "To find out the number of elements contained in a tensor, you can use \n",
    "```python\n",
    "t.numel()\n",
    "```\n",
    "This operation is particularly useful when you want to check the suitability of a new dimension for your tensor. We always want the number of elements of the source and destination tensor to be equal in order to ensure that the reshaping operation happens without an error.\n",
    "\n",
    "To reshape the tensor to a new dimension you can use\n",
    "```python\n",
    "t.reshape(m,n,p)\n",
    "```\n",
    "With the caveat that source and destination should have same numel.\n",
    "\n",
    "**Squeezing and unsqueezing** operations allow us to expand or contract the rank of a tensor. Squeezing a tensor removes the dimensions or axes that have a length of one. Unsqueezing a tensor adds a dimension with a length of 1. \n",
    "\n",
    "```python\n",
    "t.reshape([1, t.numel()]).squeeze()\n",
    "```\n",
    "\n",
    "A **Flatten** operation takes in a tensor and converts it to a 1-D tensor of number of elements in the tensor. Flattening a tensor means to remove all of it's dimensions except for 1.\n",
    "\n",
    "```python\n",
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t\n",
    "```\n",
    "\n",
    "We can concatenate one tensor alongside another either rowwise or columnwise. This can be handy when you want to collate two tensors for some reason. We can do it as follows:\n",
    "```python\n",
    "t1 = torch.tensor(data1)\n",
    "t2 = torch.tensor(data2)\n",
    "\n",
    "# Rowwise concatenation\n",
    "tcombined = torch.cat((t1, t2), dim = 0)\n",
    "\n",
    "# Columnwise concatenation\n",
    "tcombined = torch.cat((t1, t2), dim = 1)\n",
    "```\n",
    "\n",
    "A **Stacking** operation takes in a lot of tensors and adds a dimension to create a batch of tensors. eg. If we have 1000 images, we can make batches of size 64 which involve stacking 64 images together into one batch and that would be represented along an axis of the tensor. Here's how this could be achieved.\n",
    "\n",
    "```python\n",
    "# Stack 3 tensors together\n",
    "tcomb = torch.stack((t1, t2, t3))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[2., 2., 2.],\n",
      "          [2., 2., 2.],\n",
      "          [2., 2., 2.]]],\n",
      "\n",
      "\n",
      "        [[[3., 3., 3.],\n",
      "          [3., 3., 3.],\n",
      "          [3., 3., 3.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Flattening an image along color-channel component\n",
    "t1 = torch.ones((3,3))\n",
    "t2 = torch.ones((3,3)) * 2\n",
    "t3 = torch.ones((3,3)) * 3\n",
    "\n",
    "batch = torch.stack((t1, t2, t3))\n",
    "batch = batch.reshape(3, 1, 3, 3)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# We need to flatten this across the color channel and across the height and the width\n",
    "# It's easily possible by a built-in function but let's try something else\n",
    "\n",
    "batch_manual = batch.reshape(3, batch.size()[1] * batch.size()[2] * batch.size()[3])\n",
    "print(batch_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# Using a built in function\n",
    "batch_func = batch.flatten(start_dim = 1)\n",
    "print(batch_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementwise tensor operations\n",
    "\n",
    "Two elements of a tensor are said to be corresponding if they both occupy the same position in their respective tensors. This restricts these operands to both be of the same shape. \n",
    "\n",
    "Elementwise operators are the ones which operate on corresponding elements in between two tensor operands. They are also called pointwise or componentwise operations.\n",
    "\n",
    "However, this get's interesting when we have scalar values. When we operate a tensor with a scalar, it works (Well it shouldn't since the scalar is a rank 0 tensor and other tensors have non-zero rank so their shapes mismatch).\n",
    "\n",
    "What happens under the hood is that the scalar value is broadcasted to be the same size/shape as the tensor on which it is operated and then the elementwise operation is carried out.\n",
    "\n",
    "**In general, given two tensors, if the low rank tensor could be broadcasted to a high rank tensor, then the elementwise operation will happen no matter what, otherwise they won't and you'll get an error**.\n",
    "\n",
    "Apart from the general arithmetic operations there are some other operations which you should be aware of and some of them are as follows. \n",
    "\n",
    "```python\n",
    "# To check if elements of a tensor are greater than or equal to a number\n",
    "t.ge(0)\n",
    "\n",
    "# Less than or equal to a number\n",
    "t.le(0)\n",
    "\n",
    "# Greater than a number\n",
    "t.gt(0)\n",
    "\n",
    "# Less than a number\n",
    "t.lt(0)\n",
    "\n",
    "# Equal to a number\n",
    "t.eq(0)\n",
    "\n",
    "# Unary operators\n",
    "# Square root\n",
    "t.sqrt()\n",
    "\n",
    "# Modulus function\n",
    "t.abs()\n",
    "\n",
    "# Negation\n",
    "t.neg()\n",
    "\n",
    "# Chained operations\n",
    "t.sqrt().abs()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1,2],[3,4]], dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + torch.tensor(np.broadcast_to(2, t.shape), dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([1,2,3], dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-b8cac00fd286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "t + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Reduction operations\n",
    "\n",
    "They are the operations which reduce the number of elements of a tensor. There are a lot of useful tensor reduction operations which we will use commonly when dealing with tensors. Some of them are as follows:\n",
    "\n",
    "```python\n",
    "# Input is a nd tensor output is a 1D tensor with a single scalar value\n",
    "\n",
    "# Sum all the elements of a tensor \n",
    "t.sum()\n",
    "\n",
    "# Find the mean & standard deviation of all elements in the tensor\n",
    "t.mean()\n",
    "t.std()\n",
    "\n",
    "# Find the product of all the elements in the tensor\n",
    "t.prod()\n",
    "```\n",
    "\n",
    "We can reduce the tensor across a dimension in order to avoid getting one single scalar value as outputs. The meaning changes. For eg. if you mention dim = 0 in a mean operation on a 2D tensor, it will give a tensor of 1D values which will correspond to the means of all the columns in the tensor that you passed.\n",
    "\n",
    "```python\n",
    "t.sum(dim = 0)  # For a 3 row matrix, t[0] + t[1] + t[2]\n",
    "t.sum(dim = 1)  # For a 3 row matrix, [t[0].sum, t[1].sum, t[2].sum()]\n",
    "```\n",
    "\n",
    "**argmax** is an important reduction operation which returns the index of the element which has the highest value in the tensor (Or a tensor depending along a user specific dimension if you provide one).\n",
    "\n",
    "```python\n",
    "t.argmax(dim = 1)\n",
    "```\n",
    "\n",
    "If you want to access a scalar element as the result of an operation, you could do the following\n",
    "```python\n",
    "t.sum().item()\n",
    "```\n",
    "Note that the output of `t.sum()` will be returning a tensor containing a scalar value but it might not be something that you want. You just want the scalar value. That time you can use `.item` to get the scalar value.\n",
    "\n",
    "Also, if you operate along a row or column and get a 1-D tensor but want to operate on it in python, you could convert it to a list or into a numpy array and do the necessary processing.\n",
    "```python\n",
    "t.sum(axis = 1).tolist()\n",
    "t.sum(axis = 1).numpy()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
